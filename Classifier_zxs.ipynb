{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU name: Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\"GPU\" + device_lib.list_local_devices()[-1].physical_device_desc.split(\",\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order. \n",
    "import imageio # Imageio is a Python library that provides an easy interface to read and write a wide range of image data, including animated images, volumetric data, and scientific formats. \n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow.keras.layers as layers # Keras layers API\n",
    "import time\n",
    "from IPython import display # For displaying image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../input/img_align_celeba/img_align_celeba/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(images):\n",
    "#     noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "#     # take the image links and return a cropped image\n",
    "#     new_images = []\n",
    "#     for file_name in images:\n",
    "#         new_pic = load_image( path + file_name)\n",
    "#         new_images.append(new_pic)\n",
    "    \n",
    "#     images = np.array(new_images)\n",
    "#     images = images.reshape(images.shape[0], 64, 64, 3).astype('float32') # puts each number in its own numpy array so instead of [1,2,3] gonna be [[1], [2], [3]]\n",
    "#     images = (images) / 255 # normalize to [0,1]\n",
    "#     # Normalize the images to [-1. 1] so if it was 0 --> -1 and if it was 255 --> 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_image( infilename ) :\n",
    "#     img = PIL.Image.open( infilename )\n",
    "#     img = img.crop([25,65,153,193])\n",
    "#     img = img.resize((64,64))\n",
    "#     data = np.asarray( img, dtype=\"int32\" )\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image(path + \"000001.jpg\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.image.crop_to_bounding_box(image=image, \n",
    "                                        offset_height=65,\n",
    "                                        offset_width= 25,\n",
    "                                        target_height=193-65,\n",
    "                                        target_width= 153-25)\n",
    "  image = tf.image.resize(image,[64,64])\n",
    "\n",
    "  image = tf.cast(image, dtype=tf.dtypes.float32)/255  # normalize to [0,1] range\n",
    "\n",
    "  return image\n",
    "\n",
    "def load_and_preprocess_image(filename):\n",
    "  image = tf.io.read_file(filename)\n",
    "  return preprocess_image(image)\n",
    "\n",
    "img = load_and_preprocess_image(path + \"000001.jpg\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up code image pipeline\n",
    "\"\"\"\n",
    "def preprocess_image(image):\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.image.crop_to_bounding_box(image=image, \n",
    "                                        offset_height=65,\n",
    "                                        offset_width= 25,\n",
    "                                        target_height=193-65,\n",
    "                                        target_width= 153-25)\n",
    "  image = tf.image.resize(image,[64,64])\n",
    "\n",
    "  image = tf.cast(image, dtype=tf.dtypes.float32)/255  # normalize to [0,1] range\n",
    "\n",
    "  return image\n",
    "\n",
    "def load_and_preprocess_image(filename):\n",
    "  image = tf.io.read_file(filename)\n",
    "  return preprocess_image(image)\n",
    "  \n",
    "image = tf.image.decode_jpeg(tf.io.read_file(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/\"+files[0]), channels=3)\n",
    "image_c = tf.image.crop_to_bounding_box(image=image, \n",
    "                                        offset_height=65,\n",
    "                                        offset_width= 25,\n",
    "                                        target_height=193-65,\n",
    "                                        target_width= 153-25)\n",
    "plt.imshow(image_c)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_folder = '../input/'\n",
    "data = pd.read_csv(main_folder + 'list_attr_celeba.csv')\n",
    "data_2 = data.loc[:,['image_id', 'Young']]\n",
    "data_2.head()\n",
    "train_images = np.array(os.listdir(path))\n",
    "data_shuffle = data_2.sample(frac = 1).reset_index(drop = True)\n",
    "data_shuffle.replace(to_replace=-1, value=0, inplace=True)\n",
    "data_shuffle.head()\n",
    "\n",
    "BUFFER_SIZE = 20000 # number of images in training i think\n",
    "BATCH_SIZE = 200 # This is just the standard number for batch size. Google for more info\n",
    "# shuffle and batch the data\n",
    "# np.random.shuffle(train_images)\n",
    "train_images = np.split(data_shuffle['image_id'][:BUFFER_SIZE],BATCH_SIZE)\n",
    "len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>Young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>096504.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148862.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>186109.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>007346.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>040154.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id  Young\n",
       "0  096504.jpg      1\n",
       "1  148862.jpg      1\n",
       "2  186109.jpg      1\n",
       "3  007346.jpg      1\n",
       "4  040154.jpg      1"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shuffle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_d = dict(zip(data_shuffle.image_id,data_shuffle.Young))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn\n",
    "# In general, having all inputs to a neural network scaled to unit dimensions tries to convert the error surface into a more spherical shape. Hence, Gradient Descent converges faster, reducing training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "  # In Keras, you assemble layers to build models. A model is (usually) a graph of layers. The most common type of model is a stack of layers\n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  \"\"\"\n",
    "  Add a densely-connected layer to the model\n",
    "  the model will take as input arrays of shape (*, 100).\n",
    "  And and output arrays of shape (*, 7*7*256)\n",
    "  \n",
    "  after the first layer, you don't need to specify the size of the input anymore\n",
    "  Afterwards, we do automatic shape inference\n",
    "  \"\"\"\n",
    "  model.add(layers.Dense(4*4*1024, use_bias = False, input_shape = (100,)))\n",
    "  \n",
    "  \"\"\"\n",
    "  You can think about batch normalization as doing preprocessing at every layer of the network.\n",
    "  Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "  \"\"\"\n",
    "  model.add(layers.BatchNormalization())\n",
    "  \n",
    "  \"\"\"\n",
    "  ReLU is linear (identity) for all positive values, and zero for all negative values. \n",
    "  Leaky ReLU has a small slope for negative values, instead of altogether zero. For example, leaky ReLU may have y = 0.01x when x < 0\n",
    "  \"\"\"\n",
    "  model.add(layers.LeakyReLU())\n",
    "  \n",
    "  # reshape the output from something flattened to something with a shape of (7,7,256)\n",
    "  model.add(layers.Reshape(( 4, 4, 1024)))\n",
    "  assert model.output_shape == (None, 4, 4, 1024) # Note: None is the batch size\n",
    "  \n",
    "  \"\"\"\n",
    "  The generator uses a transposed convolutional layer (Upsampling) layers to produce an image from seed (random noise).\n",
    "  \n",
    "  128 is the dimensionality of the output space\n",
    "  (5,5) specifies the height and width of the 2D convolution window\n",
    "  strides = (1,1) specifies the strides of the convolution along the height and width \n",
    "  \n",
    "  \"\"\"\n",
    "  model.add(layers.Conv2DTranspose(512, (5, 5), strides = (2,2), padding = \"same\", use_bias = False))\n",
    "  assert model.output_shape == (None, 8, 8, 512)\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "  \n",
    "  # Another transposed convolutional layer (upsampling)\n",
    "  model.add(layers.Conv2DTranspose(256, (5,5), strides = (2,2), padding = \"same\", use_bias = False))\n",
    "  assert model.output_shape == (None, 16, 16, 256)\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "  \n",
    "  # Another transposed convolutional layer (upsampling)\n",
    "  model.add(layers.Conv2DTranspose(128, (5,5), strides = (2,2), padding = \"same\", use_bias = False))\n",
    "  assert model.output_shape == (None, 32, 32, 128)\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "  \n",
    "  # Final output layer also a convolutional layer (upsampling), sigmoid goes from 0 to 1\n",
    "  model.add(layers.Conv2DTranspose(3, (5,5), strides = (2,2), padding = \"same\", use_bias = False, activation = \"sigmoid\"))\n",
    "  assert model.output_shape == (None, 64, 64, 3)\n",
    "  \n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebPic(object):\n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        self.arr = None\n",
    "        self.value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb = CelebPic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb.path = \"000001.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', \n",
    "                                     input_shape=[64, 64, 3]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3)) # Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "      \n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    \"\"\"\n",
    "    Flattens the input. Does not affect the batch size.\n",
    "    If inputs are shaped (batch,) without a channel dimension, then flattening adds an extra channel dimension and output shapes are (batch, 1).\n",
    "    \"\"\"\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.4994476]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "\n",
    "\"\"\"\n",
    "Discriminator Loss\n",
    "\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predicitions on real images to an array of 1s\n",
    "and the dicriminator's predicitons on fake (generated) images to an array of 0s.\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "  total_loss = real_loss + fake_loss\n",
    "  \n",
    "  return total_loss\n",
    "\n",
    "\"\"\"\n",
    "Generator Loss\n",
    "\n",
    "The generator's loss quantifies how well it was able to trick the discrimator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1).\n",
    "Here, we will compare the discriminators decisions on the generated images to an array of 1s.\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def generator_loss(fake_output):\n",
    "  return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\"\"\"\n",
    "The discriminator and the generator optimizers are different since we will train two networks separately.\n",
    "The Adam optimization algorithm is an extension to stochastic gradient descent.\n",
    "Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.\n",
    "A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds.\n",
    "\n",
    "\"\"\"\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "                                 discriminator_optimizer = discriminator_optimizer,\n",
    "                                 discriminator = discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier) to visualize progress in the animated GIF\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_images = []\n",
    "# for file_name in train_images[0]:\n",
    "#     new_pic = load_image( path + file_name)\n",
    "#     #new_pic = tf.reshape(new_pic,[64,64,3])\n",
    "#     new_images.append(new_pic)\n",
    "    \n",
    "# images = np.array(new_images)\n",
    "# #images = tf.reshape(images.shape[0],[64,64,3])\n",
    "# images = images.reshape(images.shape[0], 64, 64, 3).astype('float32') # puts each number in its own numpy array so instead of [1,2,3] gonna be [[1], [2], [3]]\n",
    "# images = (images) / 255 # normalize to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# new_pic_tf = [tf.reshape(load_and_preprocess_image(path + file_name),[64,64,3]) for file_name in train_images[0]]\n",
    "# images_tf = tf.convert_to_tensor(new_images_tf)\n",
    "# print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# new_images_tf = []\n",
    "# for file_name in train_images[0]: #what is this train_images[0]\n",
    "#     new_pic_tf = load_and_preprocess_image( path + file_name)\n",
    "#     new_pic_tf = tf.reshape(new_pic_tf,[64,64,3])\n",
    "#     new_images_tf.append(new_pic_tf)\n",
    "# print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in celeb_d if celeb_d[x]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The training loop begins with generator receiving a random seed as input. \n",
    "That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). \n",
    "The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator.\n",
    "\"\"\"\n",
    "\n",
    "# Notice the use of tf.function\n",
    "# This annotation causes the function to be \"compiled\"\n",
    "def train_step(images): #images is the batch of filenames\n",
    "    young = [x for x in images if celeb_d[x]==1]\n",
    "    old = [x for x in images if celeb_d[x]==0]\n",
    "    \n",
    "    young_list = [tf.reshape(load_and_preprocess_image(path + file_name),[64,64,3]) for file_name in young]\n",
    "    young_tf = tf.convert_to_tensor(young_list)\n",
    "    \n",
    "    old_list = [tf.reshape(load_and_preprocess_image(path + file_name),[64,64,3]) for file_name in old]\n",
    "    old_tf = tf.convert_to_tensor(old_list)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "GradientTape() Records operations for automatic differentiation. Operations are recorded if \n",
    "they are executed within this context manager and at least one of their inputs is being \"watched\".\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      young_output = discriminator(young_tf, training=True)\n",
    "      old_output = discriminator(old_tf, training=True)\n",
    "\n",
    "      disc_loss = discriminator_loss(young_output, old_output)\n",
    "    \n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train(dataset, epochs):  \n",
    "  tf.print(\"Starting man!\")\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    tf.print(\"Starting Epoch:\", epoch)\n",
    "    batch_count = 1\n",
    "    for image_batch in dataset:\n",
    "      tf.print(\"Batch:\", batch_count)\n",
    "      train_step(image_batch)\n",
    "      print(\"Batch:\", batch_count, \"Complete\")\n",
    "      batch_count += 1\n",
    "    \n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    tf.print(\"Epoch:\", epoch, \"finished\")\n",
    "    tf.print()\n",
    "    \n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    tf.print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "    \n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False. \n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False).numpy()\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "  \n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i])\n",
    "      plt.axis('off')\n",
    "        \n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "#Image(filename='image_at_epoch_0060.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 finished\n",
      "\n",
      "Time for epoch 3 is 80.85759615898132 sec\n",
      "Starting Epoch: 3\n",
      "Batch: 1\n",
      "Batch: 1 Complete\n",
      "Batch: 2\n",
      "Batch: 2 Complete\n",
      "Batch: 3\n",
      "Batch: 3 Complete\n",
      "Batch: 4\n",
      "Batch: 4 Complete\n",
      "Batch: 5\n",
      "Batch: 5 Complete\n",
      "Batch: 6\n",
      "Batch: 6 Complete\n",
      "Batch: 7\n",
      "Batch: 7 Complete\n",
      "Batch: 8\n",
      "Batch: 8 Complete\n",
      "Batch: 9\n",
      "Batch: 9 Complete\n",
      "Batch: 10\n",
      "Batch: 10 Complete\n",
      "Batch: 11\n",
      "Batch: 11 Complete\n",
      "Batch: 12\n",
      "Batch: 12 Complete\n",
      "Batch: 13\n",
      "Batch: 13 Complete\n",
      "Batch: 14\n",
      "Batch: 14 Complete\n",
      "Batch: 15\n",
      "Batch: 15 Complete\n",
      "Batch: 16\n",
      "Batch: 16 Complete\n",
      "Batch: 17\n",
      "Batch: 17 Complete\n",
      "Batch: 18\n",
      "Batch: 18 Complete\n",
      "Batch: 19\n",
      "Batch: 19 Complete\n",
      "Batch: 20\n",
      "Batch: 20 Complete\n",
      "Batch: 21\n",
      "Batch: 21 Complete\n",
      "Batch: 22\n",
      "Batch: 22 Complete\n",
      "Batch: 23\n",
      "Batch: 23 Complete\n",
      "Batch: 24\n",
      "Batch: 24 Complete\n",
      "Batch: 25\n",
      "Batch: 25 Complete\n",
      "Batch: 26\n",
      "Batch: 26 Complete\n",
      "Batch: 27\n",
      "Batch: 27 Complete\n",
      "Batch: 28\n",
      "Batch: 28 Complete\n",
      "Batch: 29\n",
      "Batch: 29 Complete\n",
      "Batch: 30\n",
      "Batch: 30 Complete\n",
      "Batch: 31\n",
      "Batch: 31 Complete\n",
      "Batch: 32\n",
      "Batch: 32 Complete\n",
      "Batch: 33\n",
      "Batch: 33 Complete\n",
      "Batch: 34\n",
      "Batch: 34 Complete\n",
      "Batch: 35\n",
      "Batch: 35 Complete\n",
      "Batch: 36\n",
      "Batch: 36 Complete\n",
      "Batch: 37\n",
      "Batch: 37 Complete\n",
      "Batch: 38\n",
      "Batch: 38 Complete\n",
      "Batch: 39\n",
      "Batch: 39 Complete\n",
      "Batch: 40\n",
      "Batch: 40 Complete\n",
      "Batch: 41\n",
      "Batch: 41 Complete\n",
      "Batch: 42\n",
      "Batch: 42 Complete\n",
      "Batch: 43\n",
      "Batch: 43 Complete\n",
      "Batch: 44\n",
      "Batch: 44 Complete\n",
      "Batch: 45\n",
      "Batch: 45 Complete\n",
      "Batch: 46\n",
      "Batch: 46 Complete\n",
      "Batch: 47\n",
      "Batch: 47 Complete\n",
      "Batch: 48\n",
      "Batch: 48 Complete\n",
      "Batch: 49\n",
      "Batch: 49 Complete\n",
      "Batch: 50\n",
      "Batch: 50 Complete\n",
      "Batch: 51\n",
      "Batch: 51 Complete\n",
      "Batch: 52\n",
      "Batch: 52 Complete\n",
      "Batch: 53\n",
      "Batch: 53 Complete\n",
      "Batch: 54\n",
      "Batch: 54 Complete\n",
      "Batch: 55\n",
      "Batch: 55 Complete\n",
      "Batch: 56\n",
      "Batch: 56 Complete\n",
      "Batch: 57\n",
      "Batch: 57 Complete\n",
      "Batch: 58\n",
      "Batch: 58 Complete\n",
      "Batch: 59\n",
      "Batch: 59 Complete\n",
      "Batch: 60\n",
      "Batch: 60 Complete\n",
      "Batch: 61\n",
      "Batch: 61 Complete\n",
      "Batch: 62\n",
      "Batch: 62 Complete\n",
      "Batch: 63\n",
      "Batch: 63 Complete\n",
      "Batch: 64\n",
      "Batch: 64 Complete\n",
      "Batch: 65\n",
      "Batch: 65 Complete\n",
      "Batch: 66\n",
      "Batch: 66 Complete\n",
      "Batch: 67\n",
      "Batch: 67 Complete\n",
      "Batch: 68\n",
      "Batch: 68 Complete\n",
      "Batch: 69\n",
      "Batch: 69 Complete\n",
      "Batch: 70\n",
      "Batch: 70 Complete\n",
      "Batch: 71\n",
      "Batch: 71 Complete\n",
      "Batch: 72\n",
      "Batch: 72 Complete\n",
      "Batch: 73\n",
      "Batch: 73 Complete\n",
      "Batch: 74\n",
      "Batch: 74 Complete\n",
      "Batch: 75\n",
      "Batch: 75 Complete\n",
      "Batch: 76\n",
      "Batch: 76 Complete\n",
      "Batch: 77\n",
      "Batch: 77 Complete\n",
      "Batch: 78\n",
      "Batch: 78 Complete\n",
      "Batch: 79\n",
      "Batch: 79 Complete\n",
      "Batch: 80\n",
      "Batch: 80 Complete\n",
      "Batch: 81\n",
      "Batch: 81 Complete\n",
      "Batch: 82\n",
      "Batch: 82 Complete\n",
      "Batch: 83\n",
      "Batch: 83 Complete\n",
      "Batch: 84\n",
      "Batch: 84 Complete\n",
      "Batch: 85\n",
      "Batch: 85 Complete\n",
      "Batch: 86\n",
      "Batch: 86 Complete\n",
      "Batch: 87\n",
      "Batch: 87 Complete\n",
      "Batch: 88\n",
      "Batch: 88 Complete\n",
      "Batch: 89\n",
      "Batch: 89 Complete\n",
      "Batch: 90\n",
      "Batch: 90 Complete\n",
      "Batch: 91\n",
      "Batch: 91 Complete\n",
      "Batch: 92\n",
      "Batch: 92 Complete\n",
      "Batch: 93\n",
      "Batch: 93 Complete\n",
      "Batch: 94\n",
      "Batch: 94 Complete\n",
      "Batch: 95\n",
      "Batch: 95 Complete\n",
      "Batch: 96\n",
      "Batch: 96 Complete\n",
      "Batch: 97\n",
      "Batch: 97 Complete\n",
      "Batch: 98\n",
      "Batch: 98 Complete\n",
      "Batch: 99\n",
      "Batch: 99 Complete\n",
      "Batch: 100\n",
      "Batch: 100 Complete\n",
      "Batch: 101\n",
      "Batch: 101 Complete\n",
      "Batch: 102\n",
      "Batch: 102 Complete\n",
      "Batch: 103\n",
      "Batch: 103 Complete\n",
      "Batch: 104\n",
      "Batch: 104 Complete\n",
      "Batch: 105\n",
      "Batch: 105 Complete\n",
      "Batch: 106\n",
      "Batch: 106 Complete\n",
      "Batch: 107\n",
      "Batch: 107 Complete\n",
      "Batch: 108\n",
      "Batch: 108 Complete\n",
      "Batch: 109\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-3f89fd4b2177>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mbatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-275-52efea69592b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0myoung_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myoung_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mold_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mold_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-275-52efea69592b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0myoung_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myoung_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mold_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mold_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-221-09e7e82400ba>\u001b[0m in \u001b[0;36mload_and_preprocess_image\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"000001.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-221-09e7e82400ba>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      6\u001b[0m                                         \u001b[0mtarget_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m193\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                         target_width= 153-25)\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m  \u001b[0;31m# normalize to [0,1] range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mresize_images_v2\u001b[0;34m(images, size, method, preserve_aspect_ratio, antialias, name)\u001b[0m\n\u001b[1;32m   1242\u001b[0m       \u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m       skip_resize_if_same=False)\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36m_resize_images_common\u001b[0;34m(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)\u001b[0m\n\u001b[1;32m   1018\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresizer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;31m# NOTE(mrry): The shape functions for the resize ops cannot unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mresize_fn\u001b[0;34m(images_t, new_size)\u001b[0m\n\u001b[1;32m   1219\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         return gen_image_ops.resize_bilinear(\n\u001b[0;32m-> 1221\u001b[0;31m             images_t, new_size, half_pixel_centers=True)\n\u001b[0m\u001b[1;32m   1222\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mResizeMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEAREST_NEIGHBOR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       return gen_image_ops.resize_nearest_neighbor(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_image_ops.py\u001b[0m in \u001b[0;36mresize_bilinear\u001b[0;34m(images, size, align_corners, half_pixel_centers, name)\u001b[0m\n\u001b[1;32m   3178\u001b[0m         \u001b[0;34m\"ResizeBilinear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3179\u001b[0m         \u001b[0;34m\"align_corners\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"half_pixel_centers\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3180\u001b[0;31m         half_pixel_centers)\n\u001b[0m\u001b[1;32m   3181\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3182\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_images, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4))\n",
    "predictions = generator(seed, training = False).numpy()\n",
    "for i in range(seed.shape[0]):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(predictions[i], interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4))\n",
    "predictions = checkpoint.generator(seed, training = False).numpy()\n",
    "for i in range(seed.shape[0]):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(predictions[i], interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1,100]) # shape is 1, 100\n",
    "random_face = checkpoint.generator(noise, training = False).numpy()[0]\n",
    "plt.imshow(random_face, interpolation='nearest')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
